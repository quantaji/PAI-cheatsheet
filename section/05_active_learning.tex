\section{Active Learning}
\textbf{Goal} where to collect data, most useful info.

\textbf{Mutual Info} loss of uncertainty after observing $F(S):=H(f)-H(f | y_S)=I(f ; y_S)=\frac{1}{2} \log |I+\sigma^{-2} K_S|$, find $S$ that maximize, combinatorial, NP hard.

\textbf{Greedy}: $x_{t+1}=\arg \max _{x \in D} F(S_t \cup\{x\})$ $=H(f|y_{S_{t}}) - H(f|y_{S_{t+1}}) = I(f;y_{x} | y_{S_{t}})$ $=\ln(1+\sigma_{x|S_{t}}^2/\sigma_n^2)/2 =\arg \max _x \sigma_t^2(x)$
   
MI is \textit{monotone submodular}, $\forall x \in D \quad \forall A \subseteq B \subseteq D: F(A \cup\{x\})-F(A) \geq F(B \cup\{x\})-F(B)$, equiv to prove $H(y_x|y_A) \geq H(y_x|y_B)$. Info never hurts.

Constant-factor approx, near optimal $F(S_T) \geq(1-\frac{1}{e}) \max _{S \subseteq D,|S| \leq T} F(S)$

Fail if heteroscedastic, $x_{t+1} \in \arg \max _x$ $\sigma_f^2(x) /\sigma_n^2(x)$, fail to distinguish E and A uncertainty, result not most informative.

\textbf{Informative Sampling for Classification (BALD)} $x=\arg\max_x I(\theta; y_x | \mathcal{D})$ $=H(y|x, \mathcal{D}) - \mathbb{E}_{\theta{\small \sim} p(\theta|\mathcal{D})}H(y|x, \theta)$, second (both) term by Bayesian DL sampling. 

 $=-\sum_c p(y=c | \mathbf{x}, \mathcal{D}_{\text {train }}) \log p(y=c | \mathbf{x}, \mathcal{D}_{\text {train }}) $ $+ \mathbb{E}_{p(\boldsymbol{\omega} | \mathcal{D}_{\text {train }})}[\sum_c p(y=c | \mathbf{x}, \boldsymbol{\omega})$ $ \log p(y=c | \mathbf{x}, \boldsymbol{\omega})] $
$\approx-\sum_c(\frac{1}{T} \sum_t \hat{p}_c^t) \log (\frac{1}{T} \sum_t \widehat{p}_c^t)+\frac{1}{T} \sum_{c, t} \widehat{p}_c^t \log \widehat{p}_c^t$

\section{Bayesian Optimization}
\textbf{Goal}: sequentially pick $x_1,\ldots, x_T$ to find $\max_x f(x)$ with minimal samples, with some model $y_t=f(x_t)+\varepsilon_t$

\textbf{Metric} Cumulative regret $R_T=\sum_{t=1}^T(\max f(x)-f(x_t))$, if sublinear $R_T / T \to 0$, implies $\max _t f(x_t) \to f(x^*)$


\textbf{Upper confidence sampling, GP-UCB}, accusition funciton: $x_t=\arg \max _{x \in D} \mu_{t-1}(x)+\beta_t \sigma_{t-1}(x)$, $\beta_t$ for EE trade-off.


\textbf{Theorem} maximum Info gain determines regret, if choose $\beta_t$ correctly $\frac{1}{T} \sum_{t=1}^T[f(x^*)-f(x_t)]=\mathcal{O}^*(\sqrt{\frac{\gamma_T}{T}})$, where $\gamma_T=\max _{|S| \leq T} I(f ; y_S)$.

1. linear $\gamma_T=O(d \log T)$, 2. RBF $\gamma_T=O((\log T)^{d+1})$, 3. MatÃ©rn with $\nu>1/2$, $\gamma_T=O(\frac{d}{T^{\frac{d}{2 v+d}}}(\log T)^{\frac{2 v}{2 v+d}})$, UCB guarantees sublinear regret convergence.

Non-convex, optimization: low-dim Lipschitz optim, high-D gradient ascent


\textbf{expected improve}, in noise free setting  $u(f(x)):=\max (0, f(x)-f^*)$, $f^*:=\max _{i=1, \ldots, t} y_i$, $a_{\mathrm{EI}}(x)=(\mu_t(x)-f^*) \Phi(\frac{\mu_t(x)-f^*}{\sigma_t(x)})+\sigma_t(x) \phi(\frac{\mu_t(x)-f^*}{\sigma_t(x)})$
\textbf{prob of improve} (PI) $P(f(x) \geq f^*)=a_{\mathrm{PI}}(x)=\Phi(\frac{\mu_t(x)-f^*}{\sigma_t(x)})$

\textbf{Information directed sampling}
    
\textbf{Thomas Sampling}, every time, drops samples $\tilde{f} {\small \sim} P(f | x_{1: t}, y_{1: t})$, and select $x_{t+1} \in \arg \max _{x \in D} \tilde{f}(x)$. 1. randomness in sampling $\tilde{f}$ enough for EE-tradeoff, 2. similar regret bounds to UCB